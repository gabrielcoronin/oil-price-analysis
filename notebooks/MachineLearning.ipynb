{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Análise do Melhor Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Para identificar o melhor modelo para a previsão de séries temporais, testamos três abordagens distintas:\n",
    "\n",
    "LSTM (Long Short-Term Memory): Escolhemos o LSTM devido à sua capacidade de capturar dependências temporais de longo prazo e sequenciais, o que é essencial para lidar com dados de séries temporais, onde o padrão ao longo do tempo é crucial para a precisão das previsões.\n",
    "\n",
    "Gradient Boosting: Este modelo foi testado pela sua robustez e pela habilidade de modelar padrões não lineares e interações complexas em dados tabulares. Sua capacidade de melhorar o desempenho de modelos fracos, combinando várias árvores de decisão, o torna uma excelente opção quando lidamos com múltiplas variáveis e dados heterogêneos.\n",
    "\n",
    "XGBoost: Optamos também pelo XGBoost, conhecido por sua otimização e alta performance, especialmente quando lidamos com grandes volumes de dados. Além disso, sua capacidade de generalizar bem, mesmo com ajustes finos de parâmetros, é uma característica essencial para garantir precisão e evitar overfitting.\n",
    "\n",
    "Além disso, implementamos um método para verificar a correlação entre as features. Caso alguma delas apresente uma correlação superior a 90%, removemos essas variáveis para evitar o risco de overfitting, garantindo que o modelo mantenha sua generalização.\n",
    "\n",
    "Por fim, comparamos os resultados dos três modelos e salvamos aquele que apresentou o melhor desempenho, garantindo assim a escolha da abordagem mais eficaz para nosso problema de previsão."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-07 12:32:06,121 - INFO - Removed 6 highly correlated features: ['SMA_3', 'SMA_6', 'SMA_12', 'lag_1', 'lag_3', 'lag_6']\n",
      "2025-02-07 12:32:06,121 - INFO - Data loaded with 2917 records and 3 features.\n",
      "2025-02-07 12:32:06,163 - INFO - Training LSTMModel...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-07 12:32:41,342 - INFO - LSTMModel - R²: 0.9288, RMSE: 0.0131\n",
      "2025-02-07 12:32:41,343 - INFO - Training GradientBoostingModel...\n",
      "2025-02-07 12:32:44,823 - INFO - GradientBoostingModel - R²: 0.9248, RMSE: 0.0134\n",
      "2025-02-07 12:32:44,824 - INFO - Training XGBoostModel...\n",
      "2025-02-07 12:32:45,291 - INFO - XGBoostModel - R²: 0.8869, RMSE: 0.0165\n",
      "2025-02-07 12:32:45,330 - INFO - Saved best model: LSTMModel\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import xgboost as xgb\n",
    "from tensorflow.keras.models import Sequential # type: ignore\n",
    "from tensorflow.keras.layers import LSTM, Dense # type: ignore\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', force=True)\n",
    "\n",
    "def load_and_preprocess_data(url, cutoff_date):\n",
    "    try:\n",
    "        tables = pd.read_html(url, decimal=',', thousands='.')\n",
    "        df = tables[2].drop(index=0).reset_index(drop=True)\n",
    "        df.columns = ['date', 'price']\n",
    "        df['date'] = pd.to_datetime(df['date'], dayfirst=True, errors='coerce')\n",
    "        df['price'] = pd.to_numeric(df['price'], errors='coerce')\n",
    "        df.sort_values(by='date', inplace=True)\n",
    "        df.set_index('date', inplace=True)\n",
    "        df = df[df.index > cutoff_date].dropna(subset=['price']).reset_index()\n",
    "        df = create_features(df)\n",
    "        df = remove_high_correlation(df)\n",
    "        logging.info(f\"Data loaded with {df.shape[0]} records and {df.shape[1]} features.\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading data: {e}\")\n",
    "        return None\n",
    "\n",
    "def create_features(df):\n",
    "    df['SMA_3'] = df['price'].rolling(window=3).mean()\n",
    "    df['SMA_6'] = df['price'].rolling(window=6).mean()  \n",
    "    df['SMA_12'] = df['price'].rolling(window=12).mean() \n",
    "    df['pct_change'] = df['price'].pct_change()\n",
    "    df['lag_1'] = df['price'].shift(1)\n",
    "    df['lag_3'] = df['price'].shift(3)\n",
    "    df['lag_6'] = df['price'].shift(6)\n",
    "    df.dropna(inplace=True)\n",
    "    return df\n",
    "\n",
    "def remove_high_correlation(df, threshold=0.9):\n",
    "    correlation_matrix = df.corr()\n",
    "    upper_triangle = correlation_matrix.where(np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool))\n",
    "    drop_cols = [column for column in upper_triangle.columns if any(upper_triangle[column] > threshold)]\n",
    "    df = df.drop(columns=drop_cols)\n",
    "    logging.info(f\"Removed {len(drop_cols)} highly correlated features: {drop_cols}\")\n",
    "    return df\n",
    "\n",
    "def scale_data(df):\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled_data = scaler.fit_transform(df.drop(columns=['date']))\n",
    "    return scaled_data, scaler\n",
    "\n",
    "def prepare_ml_data(df, sequence_length):\n",
    "    scaled_data, scaler = scale_data(df)\n",
    "    X, y = [], []\n",
    "    for i in range(len(scaled_data) - sequence_length):\n",
    "        X.append(scaled_data[i:i+sequence_length])\n",
    "        y.append(scaled_data[i + sequence_length][0])\n",
    "    return np.array(X), np.array(y), scaler\n",
    "\n",
    "class BaseModel:\n",
    "    def train(self):\n",
    "        pass\n",
    "    \n",
    "    def predict(self):\n",
    "        pass\n",
    "    \n",
    "    def evaluate(self, y_test, predictions):\n",
    "        return {\n",
    "            \"r2\": r2_score(y_test, predictions),\n",
    "            \"mse\": mean_squared_error(y_test, predictions),\n",
    "            \"mae\": mean_absolute_error(y_test, predictions),\n",
    "            \"rmse\": np.sqrt(mean_squared_error(y_test, predictions))\n",
    "        }\n",
    "\n",
    "class LSTMModel(BaseModel):\n",
    "    def __init__(self, sequence_length, feature_size):\n",
    "        self.sequence_length = sequence_length\n",
    "        self.feature_size = feature_size\n",
    "        self.model = Sequential([ \n",
    "            LSTM(50, input_shape=(sequence_length, feature_size)),\n",
    "            Dense(1)\n",
    "        ])\n",
    "        self.model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    \n",
    "    def train(self, X_train, y_train, epochs=100, batch_size=32):\n",
    "        self.model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        return self.model.predict(X_test).flatten()\n",
    "\n",
    "class GradientBoostingModel(BaseModel):\n",
    "    def __init__(self):\n",
    "        self.model = GradientBoostingRegressor(n_estimators=200, learning_rate=0.05)\n",
    "    \n",
    "    def train(self, X_train, y_train):\n",
    "        self.model.fit(X_train.reshape(X_train.shape[0], -1), y_train)\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        return self.model.predict(X_test.reshape(X_test.shape[0], -1))\n",
    "\n",
    "class XGBoostModel(BaseModel):\n",
    "    def __init__(self):\n",
    "        self.model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=200, learning_rate=0.05)\n",
    "    \n",
    "    def train(self, X_train, y_train):\n",
    "        self.model.fit(X_train.reshape(X_train.shape[0], -1), y_train)\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        return self.model.predict(X_test.reshape(X_test.shape[0], -1))\n",
    "\n",
    "def run_pipeline(models, df, sequence_length=10):\n",
    "    X, y, scaler = prepare_ml_data(df, sequence_length)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "    best_model, best_score = None, -np.inf\n",
    "    for model in models:\n",
    "        logging.info(f\"Training {model.__class__.__name__}...\")\n",
    "        model.train(X_train, y_train)\n",
    "        predictions = model.predict(X_test)\n",
    "        metrics = model.evaluate(y_test, predictions)\n",
    "        logging.info(f\"{model.__class__.__name__} - R²: {metrics['r2']:.4f}, RMSE: {metrics['rmse']:.4f}\")\n",
    "        if metrics['r2'] > best_score:\n",
    "            best_score, best_model = metrics['r2'], model\n",
    "    return best_model, scaler\n",
    "\n",
    "def save_model(model, model_name, models_dir):\n",
    "    if isinstance(model, LSTMModel):\n",
    "        model.model.save(os.path.join(models_dir, f\"{model_name}.keras\"))\n",
    "    else:\n",
    "        joblib.dump(model.model, os.path.join(models_dir, f\"{model_name}.joblib\"))\n",
    "\n",
    "def main():\n",
    "    url = 'http://www.ipeadata.gov.br/ExibeSerie.aspx?module=m&serid=1650971490&oper=view'    \n",
    "    startDate = f\"{datetime.today().year - 10}-{datetime.today().month }-{datetime.today().day}\"\n",
    "    df = load_and_preprocess_data(url, startDate)\n",
    "    if df is None:\n",
    "        return\n",
    "    feature_size = df.shape[1] - 1\n",
    "    models = [\n",
    "        LSTMModel(sequence_length=10, feature_size=feature_size),\n",
    "        GradientBoostingModel(),\n",
    "        XGBoostModel()\n",
    "    ]   \n",
    "\n",
    "    best_model, scaler = run_pipeline(models, df)\n",
    "    if best_model:\n",
    "        model_name = best_model.__class__.__name__\n",
    "        models_dir = os.path.join(os.path.abspath(os.path.pardir), 'models')        \n",
    "        os.makedirs(models_dir, exist_ok=True)        \n",
    "        save_model(best_model, model_name, models_dir)        \n",
    "        joblib.dump(scaler, os.path.join(models_dir, \"scaler.joblib\"))        \n",
    "        logging.info(f\"Saved best model: {model_name}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
